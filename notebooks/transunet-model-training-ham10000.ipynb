{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "924b8641",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-11-21T07:16:29.241296Z",
     "iopub.status.busy": "2025-11-21T07:16:29.241018Z",
     "iopub.status.idle": "2025-11-21T18:55:57.220912Z",
     "shell.execute_reply": "2025-11-21T18:55:57.219969Z"
    },
    "papermill": {
     "duration": 41967.987938,
     "end_time": "2025-11-21T18:55:57.225068",
     "exception": false,
     "start_time": "2025-11-21T07:16:29.237130",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Total images: 10015\n",
      "Train: 8012, Val: 2003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/albumentations/core/validation.py:114: UserWarning: ShiftScaleRotate is a special case of Affine transform. Please use Affine transform instead.\n",
      "  original_init(self, **validated_kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 81.428417 M\n",
      "Epoch 01/50 | Train Loss: 0.6331 | Val Loss: 0.8105 | Val Dice: 0.7331 | Val IoU: 0.6108 | Val Acc: 0.8304\n",
      "Epoch 02/50 | Train Loss: 0.3900 | Val Loss: 0.3082 | Val Dice: 0.8901 | Val IoU: 0.8186 | Val Acc: 0.9447\n",
      "Epoch 03/50 | Train Loss: 0.3194 | Val Loss: 0.3376 | Val Dice: 0.8773 | Val IoU: 0.8038 | Val Acc: 0.9401\n",
      "Epoch 04/50 | Train Loss: 0.3726 | Val Loss: 0.4055 | Val Dice: 0.8549 | Val IoU: 0.7763 | Val Acc: 0.9192\n",
      "Epoch 05/50 | Train Loss: 0.3770 | Val Loss: 0.3672 | Val Dice: 0.8638 | Val IoU: 0.7865 | Val Acc: 0.9288\n",
      "Epoch 06/50 | Train Loss: 0.3858 | Val Loss: 0.3671 | Val Dice: 0.8610 | Val IoU: 0.7829 | Val Acc: 0.9293\n",
      "Epoch 07/50 | Train Loss: 0.3545 | Val Loss: 0.3243 | Val Dice: 0.8779 | Val IoU: 0.8060 | Val Acc: 0.9385\n",
      "Epoch 08/50 | Train Loss: 0.3380 | Val Loss: 0.3173 | Val Dice: 0.8803 | Val IoU: 0.8074 | Val Acc: 0.9399\n",
      "Epoch 09/50 | Train Loss: 0.3518 | Val Loss: 0.3201 | Val Dice: 0.8794 | Val IoU: 0.8079 | Val Acc: 0.9378\n",
      "Epoch 10/50 | Train Loss: 0.3299 | Val Loss: 0.3151 | Val Dice: 0.8808 | Val IoU: 0.8095 | Val Acc: 0.9395\n",
      "Epoch 11/50 | Train Loss: 0.3216 | Val Loss: 0.3281 | Val Dice: 0.8779 | Val IoU: 0.8042 | Val Acc: 0.9376\n",
      "Epoch 12/50 | Train Loss: 0.3078 | Val Loss: 0.3042 | Val Dice: 0.8866 | Val IoU: 0.8197 | Val Acc: 0.9420\n",
      "Epoch 13/50 | Train Loss: 0.2999 | Val Loss: 0.3253 | Val Dice: 0.8792 | Val IoU: 0.8094 | Val Acc: 0.9358\n",
      "Epoch 14/50 | Train Loss: 0.2961 | Val Loss: 0.3059 | Val Dice: 0.8880 | Val IoU: 0.8165 | Val Acc: 0.9425\n",
      "Epoch 15/50 | Train Loss: 0.2928 | Val Loss: 0.2877 | Val Dice: 0.8927 | Val IoU: 0.8261 | Val Acc: 0.9449\n",
      "Epoch 16/50 | Train Loss: 0.2866 | Val Loss: 0.2914 | Val Dice: 0.8886 | Val IoU: 0.8209 | Val Acc: 0.9439\n",
      "Epoch 17/50 | Train Loss: 0.2825 | Val Loss: 0.2746 | Val Dice: 0.8985 | Val IoU: 0.8335 | Val Acc: 0.9470\n",
      "Epoch 18/50 | Train Loss: 0.2787 | Val Loss: 0.2692 | Val Dice: 0.8998 | Val IoU: 0.8359 | Val Acc: 0.9482\n",
      "Epoch 19/50 | Train Loss: 0.2780 | Val Loss: 0.3057 | Val Dice: 0.8868 | Val IoU: 0.8180 | Val Acc: 0.9407\n",
      "Epoch 20/50 | Train Loss: 0.3001 | Val Loss: 0.2806 | Val Dice: 0.8948 | Val IoU: 0.8298 | Val Acc: 0.9461\n",
      "Epoch 21/50 | Train Loss: 0.2816 | Val Loss: 0.2848 | Val Dice: 0.8911 | Val IoU: 0.8230 | Val Acc: 0.9445\n",
      "Epoch 22/50 | Train Loss: 0.2705 | Val Loss: 0.2864 | Val Dice: 0.8943 | Val IoU: 0.8282 | Val Acc: 0.9463\n",
      "Epoch 23/50 | Train Loss: 0.2944 | Val Loss: 0.2941 | Val Dice: 0.8924 | Val IoU: 0.8252 | Val Acc: 0.9440\n",
      "Epoch 24/50 | Train Loss: 0.2765 | Val Loss: 0.2924 | Val Dice: 0.8889 | Val IoU: 0.8200 | Val Acc: 0.9442\n",
      "Epoch 25/50 | Train Loss: 0.2903 | Val Loss: 0.2710 | Val Dice: 0.8980 | Val IoU: 0.8346 | Val Acc: 0.9480\n",
      "Epoch 26/50 | Train Loss: 0.2646 | Val Loss: 0.2639 | Val Dice: 0.9014 | Val IoU: 0.8381 | Val Acc: 0.9493\n",
      "Epoch 27/50 | Train Loss: 0.2676 | Val Loss: 0.2856 | Val Dice: 0.8946 | Val IoU: 0.8296 | Val Acc: 0.9453\n",
      "Epoch 28/50 | Train Loss: 0.2685 | Val Loss: 0.2608 | Val Dice: 0.9029 | Val IoU: 0.8404 | Val Acc: 0.9499\n",
      "Epoch 29/50 | Train Loss: 0.2611 | Val Loss: 0.2575 | Val Dice: 0.9055 | Val IoU: 0.8449 | Val Acc: 0.9503\n",
      "Epoch 30/50 | Train Loss: 0.2648 | Val Loss: 0.2692 | Val Dice: 0.8995 | Val IoU: 0.8346 | Val Acc: 0.9483\n",
      "Epoch 31/50 | Train Loss: 0.2594 | Val Loss: 0.2641 | Val Dice: 0.9013 | Val IoU: 0.8385 | Val Acc: 0.9488\n",
      "Epoch 32/50 | Train Loss: 0.2558 | Val Loss: 0.2483 | Val Dice: 0.9091 | Val IoU: 0.8503 | Val Acc: 0.9519\n",
      "Epoch 33/50 | Train Loss: 0.2543 | Val Loss: 0.2440 | Val Dice: 0.9105 | Val IoU: 0.8520 | Val Acc: 0.9529\n",
      "Epoch 34/50 | Train Loss: 0.2520 | Val Loss: 0.2564 | Val Dice: 0.9050 | Val IoU: 0.8429 | Val Acc: 0.9502\n",
      "Epoch 35/50 | Train Loss: 0.2544 | Val Loss: 0.2540 | Val Dice: 0.9048 | Val IoU: 0.8434 | Val Acc: 0.9511\n",
      "Epoch 36/50 | Train Loss: 0.2524 | Val Loss: 0.2498 | Val Dice: 0.9093 | Val IoU: 0.8504 | Val Acc: 0.9518\n",
      "Epoch 37/50 | Train Loss: 0.2572 | Val Loss: 0.2509 | Val Dice: 0.9070 | Val IoU: 0.8472 | Val Acc: 0.9519\n",
      "Epoch 38/50 | Train Loss: 0.2640 | Val Loss: 0.2846 | Val Dice: 0.8971 | Val IoU: 0.8315 | Val Acc: 0.9466\n",
      "Epoch 39/50 | Train Loss: 0.2559 | Val Loss: 0.2509 | Val Dice: 0.9072 | Val IoU: 0.8474 | Val Acc: 0.9515\n",
      "Epoch 40/50 | Train Loss: 0.2466 | Val Loss: 0.2506 | Val Dice: 0.9085 | Val IoU: 0.8486 | Val Acc: 0.9517\n",
      "Epoch 41/50 | Train Loss: 0.2445 | Val Loss: 0.2419 | Val Dice: 0.9116 | Val IoU: 0.8532 | Val Acc: 0.9532\n",
      "Epoch 42/50 | Train Loss: 0.2438 | Val Loss: 0.2464 | Val Dice: 0.9080 | Val IoU: 0.8488 | Val Acc: 0.9522\n",
      "Epoch 43/50 | Train Loss: 0.2445 | Val Loss: 0.2444 | Val Dice: 0.9110 | Val IoU: 0.8534 | Val Acc: 0.9530\n",
      "Epoch 44/50 | Train Loss: 0.2406 | Val Loss: 0.2419 | Val Dice: 0.9112 | Val IoU: 0.8530 | Val Acc: 0.9533\n",
      "Epoch 45/50 | Train Loss: 0.2411 | Val Loss: 0.2356 | Val Dice: 0.9127 | Val IoU: 0.8553 | Val Acc: 0.9543\n",
      "Epoch 46/50 | Train Loss: 0.2471 | Val Loss: 0.2557 | Val Dice: 0.9055 | Val IoU: 0.8446 | Val Acc: 0.9506\n",
      "Epoch 47/50 | Train Loss: 0.2397 | Val Loss: 0.2342 | Val Dice: 0.9142 | Val IoU: 0.8576 | Val Acc: 0.9549\n",
      "Epoch 48/50 | Train Loss: 0.2369 | Val Loss: 0.2399 | Val Dice: 0.9123 | Val IoU: 0.8552 | Val Acc: 0.9541\n",
      "Epoch 49/50 | Train Loss: 0.2433 | Val Loss: 0.2615 | Val Dice: 0.9029 | Val IoU: 0.8410 | Val Acc: 0.9492\n",
      "Epoch 50/50 | Train Loss: 0.2394 | Val Loss: 0.2418 | Val Dice: 0.9111 | Val IoU: 0.8524 | Val Acc: 0.9533\n",
      "Saved training history to: /kaggle/working/ham_transunet_outputs/training_history.csv\n",
      "Saved loss curves to: /kaggle/working/ham_transunet_outputs/loss_curves.png\n",
      "Saved dice curves to: /kaggle/working/ham_transunet_outputs/dice_curves.png\n",
      "Final Val metrics: {'loss': 0.23417326392405535, 'dice': 0.914194470145313, 'iou': 0.8576381161393397, 'acc': 0.9548642537983291}\n",
      "Confusion matrix:\n",
      " [[18827372   402982]\n",
      " [  819548  6164498]]\n",
      "Saved confusion matrix plot to: /kaggle/working/ham_transunet_outputs/confusion_matrix.png\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "  Background       0.96      0.98      0.97  19230354\n",
      "      Lesion       0.94      0.88      0.91   6984046\n",
      "\n",
      "    accuracy                           0.95  26214400\n",
      "   macro avg       0.95      0.93      0.94  26214400\n",
      "weighted avg       0.95      0.95      0.95  26214400\n",
      "\n",
      "Saved classification report to: /kaggle/working/ham_transunet_outputs/classification_report.txt\n",
      "All done.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# TransUNet-AEO style segmentation on HAM10000\n",
    "# Dataset structure:\n",
    "#   /kaggle/input/ham1000-segmentation-and-classification/\n",
    "#       images/\n",
    "#       masks/\n",
    "#       GroundTruth.csv   (not needed for segmentation)\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# If albumentations is not installed, uncomment:\n",
    "# !pip install -q albumentations==1.4.3\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# ----------------- CONFIG -----------------\n",
    "DATA_ROOT = Path(\"/kaggle/input/ham1000-segmentation-and-classification\")\n",
    "IMAGES_DIR = DATA_ROOT / \"images\"\n",
    "MASKS_DIR = DATA_ROOT / \"masks\"\n",
    "\n",
    "OUTPUT_DIR = Path(\"/kaggle/working/ham_transunet_outputs\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "IMG_SIZE = 256\n",
    "BATCH_SIZE = 8\n",
    "NUM_EPOCHS = 50\n",
    "LR = 1e-4\n",
    "VAL_SPLIT = 0.2\n",
    "SEED = 42\n",
    "NUM_WORKERS = 2\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(\"Using device:\", DEVICE)\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if DEVICE == \"cuda\":\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# ----------------- DATASET -----------------\n",
    "\n",
    "class HAMSegmentationDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Loads image + mask pairs for lesion segmentation.\n",
    "    Assumes:\n",
    "        images: <id>.jpg\n",
    "        masks : <id>_segmentation.png\n",
    "    \"\"\"\n",
    "    def __init__(self, image_ids, img_dir, mask_dir, transforms=None):\n",
    "        self.image_ids = image_ids\n",
    "        self.img_dir = img_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.image_ids[idx]\n",
    "\n",
    "        img_path = self.img_dir / f\"{img_id}.jpg\"\n",
    "        mask_path = self.mask_dir / f\"{img_id}_segmentation.png\"\n",
    "\n",
    "        image = np.array(Image.open(img_path).convert(\"RGB\"))\n",
    "        mask = np.array(Image.open(mask_path).convert(\"L\"))\n",
    "\n",
    "        # ensure binary mask 0/1\n",
    "        mask = (mask > 0).astype(\"float32\")\n",
    "\n",
    "        # Albumentations expects HWC\n",
    "        if self.transforms:\n",
    "            augmented = self.transforms(image=image, mask=mask)\n",
    "            image = augmented[\"image\"]          # tensor CxHxW\n",
    "            mask = augmented[\"mask\"].unsqueeze(0)  # 1xHxW\n",
    "        else:\n",
    "            # fallback: simple tensor conversion\n",
    "            image = torch.from_numpy(image).permute(2, 0, 1).float() / 255.0\n",
    "            mask = torch.from_numpy(mask).unsqueeze(0).float()\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "\n",
    "# --------------- TRANSFORMS ---------------\n",
    "\n",
    "train_transforms = A.Compose(\n",
    "    [\n",
    "        A.Resize(IMG_SIZE, IMG_SIZE),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.5),\n",
    "        A.RandomRotate90(p=0.5),\n",
    "        A.ShiftScaleRotate(\n",
    "            shift_limit=0.05, scale_limit=0.1, rotate_limit=15, p=0.5,\n",
    "            border_mode=0\n",
    "        ),\n",
    "        A.GaussNoise(p=0.2),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406),\n",
    "                    std=(0.229, 0.224, 0.225)),\n",
    "        ToTensorV2(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "val_transforms = A.Compose(\n",
    "    [\n",
    "        A.Resize(IMG_SIZE, IMG_SIZE),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406),\n",
    "                    std=(0.229, 0.224, 0.225)),\n",
    "        ToTensorV2(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# --------------- COLLECT IMAGE IDS ---------------\n",
    "\n",
    "# image filenames are something like ISIC_0024306.jpg\n",
    "all_img_files = sorted([f for f in IMAGES_DIR.iterdir() if f.suffix.lower() == \".jpg\"])\n",
    "all_ids = [f.stem for f in all_img_files]\n",
    "\n",
    "train_ids, val_ids = train_test_split(\n",
    "    all_ids, test_size=VAL_SPLIT, random_state=SEED\n",
    ")\n",
    "\n",
    "print(f\"Total images: {len(all_ids)}\")\n",
    "print(f\"Train: {len(train_ids)}, Val: {len(val_ids)}\")\n",
    "\n",
    "train_dataset = HAMSegmentationDataset(train_ids, IMAGES_DIR, MASKS_DIR, train_transforms)\n",
    "val_dataset   = HAMSegmentationDataset(val_ids,   IMAGES_DIR, MASKS_DIR, val_transforms)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
    "    num_workers=NUM_WORKERS, pin_memory=True\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "    num_workers=NUM_WORKERS, pin_memory=True\n",
    ")\n",
    "\n",
    "# --------------- MODEL (TransUNet-like) ---------------\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "\n",
    "class UpBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.up = nn.ConvTranspose2d(in_ch, out_ch, kernel_size=2, stride=2)\n",
    "        self.conv = ConvBlock(in_ch, out_ch)\n",
    "\n",
    "    def forward(self, x, skip):\n",
    "        x = self.up(x)\n",
    "        # handle size mismatch due to rounding\n",
    "        if x.size(-1) != skip.size(-1) or x.size(-2) != skip.size(-2):\n",
    "            diff_y = skip.size(-2) - x.size(-2)\n",
    "            diff_x = skip.size(-1) - x.size(-1)\n",
    "            x = F.pad(x, [diff_x // 2, diff_x - diff_x // 2,\n",
    "                          diff_y // 2, diff_y - diff_y // 2])\n",
    "        x = torch.cat([skip, x], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class TransUNetAEO(nn.Module):\n",
    "    \"\"\"\n",
    "    TransUNet-style UNet with transformer bottleneck.\n",
    "    'AEO' (optimizer) is not embedded here; you would tune hyperparams outside.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 img_size=256,\n",
    "                 in_ch=3,\n",
    "                 num_classes=1,\n",
    "                 base_ch=64,\n",
    "                 num_heads=4,\n",
    "                 transformer_layers=4,\n",
    "                 dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.enc1 = ConvBlock(in_ch, base_ch)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "\n",
    "        self.enc2 = ConvBlock(base_ch, base_ch * 2)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "\n",
    "        self.enc3 = ConvBlock(base_ch * 2, base_ch * 4)\n",
    "        self.pool3 = nn.MaxPool2d(2)\n",
    "\n",
    "        self.enc4 = ConvBlock(base_ch * 4, base_ch * 8)\n",
    "        self.pool4 = nn.MaxPool2d(2)\n",
    "\n",
    "        bottleneck_ch = base_ch * 16\n",
    "        self.bottleneck_conv = ConvBlock(base_ch * 8, bottleneck_ch)\n",
    "\n",
    "        # Transformer encoder at bottleneck\n",
    "        H = img_size // 16\n",
    "        W = img_size // 16\n",
    "        self.num_tokens = H * W\n",
    "        d_model = bottleneck_ch\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=d_model * 4,\n",
    "            dropout=dropout,\n",
    "            batch_first=False,   # (S, B, E)\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            encoder_layer,\n",
    "            num_layers=transformer_layers\n",
    "        )\n",
    "\n",
    "        # Decoder\n",
    "        self.up4 = UpBlock(bottleneck_ch, base_ch * 8)\n",
    "        self.up3 = UpBlock(base_ch * 8, base_ch * 4)\n",
    "        self.up2 = UpBlock(base_ch * 4, base_ch * 2)\n",
    "        self.up1 = UpBlock(base_ch * 2, base_ch)\n",
    "\n",
    "        self.final_conv = nn.Conv2d(base_ch, num_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        e1 = self.enc1(x)\n",
    "        p1 = self.pool1(e1)\n",
    "\n",
    "        e2 = self.enc2(p1)\n",
    "        p2 = self.pool2(e2)\n",
    "\n",
    "        e3 = self.enc3(p2)\n",
    "        p3 = self.pool3(e3)\n",
    "\n",
    "        e4 = self.enc4(p3)\n",
    "        p4 = self.pool4(e4)\n",
    "\n",
    "        b = self.bottleneck_conv(p4)  # B x C x H x W\n",
    "\n",
    "        B, C, H, W = b.shape\n",
    "        seq = b.flatten(2).permute(2, 0, 1)   # (S, B, C)\n",
    "        seq = self.transformer(seq)\n",
    "        b = seq.permute(1, 2, 0).view(B, C, H, W)\n",
    "\n",
    "        d4 = self.up4(b, e4)\n",
    "        d3 = self.up3(d4, e3)\n",
    "        d2 = self.up2(d3, e2)\n",
    "        d1 = self.up1(d2, e1)\n",
    "\n",
    "        out = self.final_conv(d1)  # logits\n",
    "        return out\n",
    "\n",
    "\n",
    "# --------------- LOSS & METRICS ---------------\n",
    "\n",
    "class DiceBCELoss(nn.Module):\n",
    "    def __init__(self, smooth=1.0):\n",
    "        super().__init__()\n",
    "        self.bce = nn.BCEWithLogitsLoss()\n",
    "        self.smooth = smooth\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        bce = self.bce(logits, targets)\n",
    "        probs = torch.sigmoid(logits)\n",
    "        num = 2 * (probs * targets).sum(dim=(2, 3)) + self.smooth\n",
    "        den = probs.sum(dim=(2, 3)) + targets.sum(dim=(2, 3)) + self.smooth\n",
    "        dice_loss = 1 - (num / den).mean()\n",
    "        return bce + dice_loss\n",
    "\n",
    "\n",
    "def dice_coef(logits, targets, smooth=1.0):\n",
    "    probs = torch.sigmoid(logits)\n",
    "    probs = (probs > 0.5).float()\n",
    "    num = 2 * (probs * targets).sum(dim=(2, 3)) + smooth\n",
    "    den = probs.sum(dim=(2, 3)) + targets.sum(dim=(2, 3)) + smooth\n",
    "    return (num / den).mean().item()\n",
    "\n",
    "\n",
    "def iou_score(logits, targets, smooth=1.0):\n",
    "    probs = torch.sigmoid(logits)\n",
    "    probs = (probs > 0.5).float()\n",
    "    intersection = (probs * targets).sum(dim=(2, 3))\n",
    "    union = probs.sum(dim=(2, 3)) + targets.sum(dim=(2, 3)) - intersection\n",
    "    iou = (intersection + smooth) / (union + smooth)\n",
    "    return iou.mean().item()\n",
    "\n",
    "\n",
    "def pixel_accuracy(logits, targets):\n",
    "    probs = torch.sigmoid(logits)\n",
    "    preds = (probs > 0.5).float()\n",
    "    correct = (preds == targets).float().mean()\n",
    "    return correct.item()\n",
    "\n",
    "# --------------- TRAIN / EVAL LOOPS ---------------\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, loss_fn):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_dice = 0.0\n",
    "    running_iou = 0.0\n",
    "    running_acc = 0.0\n",
    "    n_batches = 0\n",
    "\n",
    "    for imgs, masks in loader:\n",
    "        imgs = imgs.to(DEVICE)\n",
    "        masks = masks.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(imgs)\n",
    "        loss = loss_fn(logits, masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        running_dice += dice_coef(logits, masks)\n",
    "        running_iou += iou_score(logits, masks)\n",
    "        running_acc += pixel_accuracy(logits, masks)\n",
    "        n_batches += 1\n",
    "\n",
    "    return {\n",
    "        \"loss\": running_loss / n_batches,\n",
    "        \"dice\": running_dice / n_batches,\n",
    "        \"iou\": running_iou / n_batches,\n",
    "        \"acc\": running_acc / n_batches,\n",
    "    }\n",
    "\n",
    "\n",
    "def evaluate(model, loader, loss_fn, collect_for_cm=False, max_batches_cm=30):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    running_dice = 0.0\n",
    "    running_iou = 0.0\n",
    "    running_acc = 0.0\n",
    "    n_batches = 0\n",
    "\n",
    "    y_true_all = []\n",
    "    y_pred_all = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (imgs, masks) in enumerate(loader):\n",
    "            imgs = imgs.to(DEVICE)\n",
    "            masks = masks.to(DEVICE)\n",
    "\n",
    "            logits = model(imgs)\n",
    "            loss = loss_fn(logits, masks)\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            running_dice += dice_coef(logits, masks)\n",
    "            running_iou += iou_score(logits, masks)\n",
    "            running_acc += pixel_accuracy(logits, masks)\n",
    "            n_batches += 1\n",
    "\n",
    "            if collect_for_cm and batch_idx < max_batches_cm:\n",
    "                probs = torch.sigmoid(logits)\n",
    "                preds = (probs > 0.5).float()\n",
    "                # flatten\n",
    "                y_true_all.append(masks.cpu().numpy().ravel())\n",
    "                y_pred_all.append(preds.cpu().numpy().ravel())\n",
    "\n",
    "    metrics = {\n",
    "        \"loss\": running_loss / n_batches,\n",
    "        \"dice\": running_dice / n_batches,\n",
    "        \"iou\": running_iou / n_batches,\n",
    "        \"acc\": running_acc / n_batches,\n",
    "    }\n",
    "\n",
    "    if collect_for_cm and len(y_true_all) > 0:\n",
    "        y_true_all = np.concatenate(y_true_all)\n",
    "        y_pred_all = np.concatenate(y_pred_all)\n",
    "        return metrics, y_true_all, y_pred_all\n",
    "    else:\n",
    "        return metrics, None, None\n",
    "\n",
    "\n",
    "# --------------- INITIALIZE ---------------\n",
    "\n",
    "model = TransUNetAEO(\n",
    "    img_size=IMG_SIZE,\n",
    "    in_ch=3,\n",
    "    num_classes=1,\n",
    "    base_ch=64,\n",
    "    num_heads=4,\n",
    "    transformer_layers=4,\n",
    "    dropout=0.1,\n",
    ").to(DEVICE)\n",
    "\n",
    "loss_fn = DiceBCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "print(\"Model parameters:\", sum(p.numel() for p in model.parameters()) / 1e6, \"M\")\n",
    "\n",
    "# --------------- TRAINING LOOP ---------------\n",
    "\n",
    "history = []\n",
    "\n",
    "best_val_dice = 0.0\n",
    "best_model_path = OUTPUT_DIR / \"best_transunet_aeo.pth\"\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    train_metrics = train_one_epoch(model, train_loader, optimizer, loss_fn)\n",
    "    val_metrics, _, _ = evaluate(model, val_loader, loss_fn, collect_for_cm=False)\n",
    "\n",
    "    epoch_info = {\n",
    "        \"epoch\": epoch,\n",
    "        \"train_loss\": train_metrics[\"loss\"],\n",
    "        \"train_dice\": train_metrics[\"dice\"],\n",
    "        \"train_iou\": train_metrics[\"iou\"],\n",
    "        \"train_acc\": train_metrics[\"acc\"],\n",
    "        \"val_loss\": val_metrics[\"loss\"],\n",
    "        \"val_dice\": val_metrics[\"dice\"],\n",
    "        \"val_iou\": val_metrics[\"iou\"],\n",
    "        \"val_acc\": val_metrics[\"acc\"],\n",
    "    }\n",
    "    history.append(epoch_info)\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch:02d}/{NUM_EPOCHS} | \"\n",
    "        f\"Train Loss: {train_metrics['loss']:.4f} | Val Loss: {val_metrics['loss']:.4f} | \"\n",
    "        f\"Val Dice: {val_metrics['dice']:.4f} | Val IoU: {val_metrics['iou']:.4f} | \"\n",
    "        f\"Val Acc: {val_metrics['acc']:.4f}\"\n",
    "    )\n",
    "\n",
    "    # save best model by Dice\n",
    "    if val_metrics[\"dice\"] > best_val_dice:\n",
    "        best_val_dice = val_metrics[\"dice\"]\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "\n",
    "# Save training history\n",
    "history_df = pd.DataFrame(history)\n",
    "history_csv_path = OUTPUT_DIR / \"training_history.csv\"\n",
    "history_df.to_csv(history_csv_path, index=False)\n",
    "print(\"Saved training history to:\", history_csv_path)\n",
    "\n",
    "# --------------- PLOTS (Loss & Dice) ---------------\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(history_df[\"epoch\"], history_df[\"train_loss\"], label=\"Train Loss\")\n",
    "plt.plot(history_df[\"epoch\"], history_df[\"val_loss\"], label=\"Val Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss Curves\")\n",
    "plt.legend()\n",
    "loss_fig_path = OUTPUT_DIR / \"loss_curves.png\"\n",
    "plt.savefig(loss_fig_path, dpi=150, bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "print(\"Saved loss curves to:\", loss_fig_path)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(history_df[\"epoch\"], history_df[\"train_dice\"], label=\"Train Dice\")\n",
    "plt.plot(history_df[\"epoch\"], history_df[\"val_dice\"], label=\"Val Dice\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Dice Coefficient\")\n",
    "plt.title(\"Dice Curves\")\n",
    "plt.legend()\n",
    "dice_fig_path = OUTPUT_DIR / \"dice_curves.png\"\n",
    "plt.savefig(dice_fig_path, dpi=150, bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "print(\"Saved dice curves to:\", dice_fig_path)\n",
    "\n",
    "# --------------- FINAL EVAL: CONFUSION MATRIX + CLASSIF REPORT ---------------\n",
    "\n",
    "# load best model\n",
    "model.load_state_dict(torch.load(best_model_path, map_location=DEVICE))\n",
    "\n",
    "val_metrics, y_true, y_pred = evaluate(\n",
    "    model, val_loader, loss_fn, collect_for_cm=True, max_batches_cm=50\n",
    ")\n",
    "\n",
    "print(\"Final Val metrics:\", val_metrics)\n",
    "\n",
    "# y_true / y_pred are pixel-wise labels (0/1), sampled from up to 50 batches\n",
    "if y_true is not None:\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "    print(\"Confusion matrix:\\n\", cm)\n",
    "\n",
    "    cm_fig_path = OUTPUT_DIR / \"confusion_matrix.png\"\n",
    "    plt.figure()\n",
    "    plt.imshow(cm, interpolation=\"nearest\")\n",
    "    plt.title(\"Pixel-wise Confusion Matrix\")\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(2)\n",
    "    plt.xticks(tick_marks, [\"Background\", \"Lesion\"])\n",
    "    plt.yticks(tick_marks, [\"Background\", \"Lesion\"])\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "\n",
    "    # annotate cells\n",
    "    thresh = cm.max() / 2.0\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            plt.text(\n",
    "                j, i, format(cm[i, j], \"d\"),\n",
    "                horizontalalignment=\"center\",\n",
    "                color=\"white\" if cm[i, j] > thresh else \"black\",\n",
    "            )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(cm_fig_path, dpi=150)\n",
    "    plt.close()\n",
    "    print(\"Saved confusion matrix plot to:\", cm_fig_path)\n",
    "\n",
    "    # classification report\n",
    "    clf_report = classification_report(\n",
    "        y_true, y_pred,\n",
    "        target_names=[\"Background\", \"Lesion\"]\n",
    "    )\n",
    "    print(\"Classification report:\\n\", clf_report)\n",
    "\n",
    "    report_path = OUTPUT_DIR / \"classification_report.txt\"\n",
    "    with open(report_path, \"w\") as f:\n",
    "        f.write(clf_report)\n",
    "    print(\"Saved classification report to:\", report_path)\n",
    "\n",
    "print(\"All done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786a34ed",
   "metadata": {
    "papermill": {
     "duration": 0.003555,
     "end_time": "2025-11-21T18:55:57.232327",
     "exception": false,
     "start_time": "2025-11-21T18:55:57.228772",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 1370616,
     "sourceId": 2275763,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 41974.408931,
   "end_time": "2025-11-21T18:55:59.675021",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-11-21T07:16:25.266090",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
